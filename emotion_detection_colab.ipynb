{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "git push origin main clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "cd strella-clone && python -m http.server 8080 speech-emotion-detection\n",
    "\n",
    "# Install dependencies\n",
    "pip install reportlab install -q numpy pandas scikit-learn matplotlib tensorflow librosa transformers soundfile"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/speech-emotion-detection\")\n",
    "\n",
    "from src.audio_features import AudioFeatureExtractor\n",
    "from src.data_processor import DataProcessor\n",
    "from src.models.audio_model import AudioEmotionModel\n",
    "from src.models.text_model import TextEmotionModel\n",
    "from src.models.multimodal_analyzer import MultimodalAnalyzer\n",
    "from src.utils import setup_logging, load_config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load and show configuration\n",
    "config = load_config(\"/content/speech-emotion-detection/config.yaml\")\n",
    "print(config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Process audio data\n",
    "data_processor = DataProcessor(config)\n",
    "features, labels = data_processor.process_data()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processor.split_data(features, labels)\n",
    "\n",
    "# Display data info\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize and train the audio model\n",
    "audio_model = AudioEmotionModel(config)\n",
    "history = audio_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Text Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize text model\n",
    "text_model = TextEmotionModel(config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize the multimodal analyzer\n",
    "analyzer = MultimodalAnalyzer(audio_model, text_model, config)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test with sample audio\n",
    "audio_path = \"/content/speech-emotion-detection/data/ravdess/Actor_01/03-01-01-01-01-01-01.wav\"\n",
    "text = \"I'm feeling quite happy today.\"\n",
    "\n",
    "# Extract audio features\n",
    "feature_extractor = AudioFeatureExtractor(config)\n",
    "audio_features = feature_extractor.extract_features(audio_path)\n",
    "\n",
    "# Run multimodal analysis\n",
    "result = analyzer.analyze(audio_features, text)\n",
    "print(\"\\nMultimodal Analysis Results:\")\n",
    "print(f\"Detected Emotion: {result[\"emotion\"]}\")\n",
    "print(f\"Audio Emotion: {result[\"audio_emotion\"]}\")\n",
    "print(f\"Text Emotion: {result[\"text_emotion\"]}\")\n",
    "print(f\"Confidence: {result[\"confidence\"]:.2f}\")\n",
    "print(f\"Modality Agreement: {result[\"modality_agreement\"]}\")\n",
    "print(f\"Sarcasm Detected: {result[\"sarcasm_detected\"]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the audio model\n",
    "audio_model.evaluate(X_test, y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Save audio model\n",
    "audio_model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Models saved to Google Drive.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}