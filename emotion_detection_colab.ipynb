{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59de80d0",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda614de",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282dca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "\n",
    "# Change working directory to the cloned repo\n",
    "import os\n",
    "os.chdir('speech-emotion-detection')\n",
    "!pwd\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q numpy pandas scikit-learn matplotlib tensorflow librosa transformers soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abba474",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS dataset\n",
    "!wget -O ravdess.zip https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
    "!mkdir -p data/ravdess\n",
    "!unzip -q ravdess.zip -d data/ravdess\n",
    "!rm ravdess.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ba8ec",
   "metadata": {},
   "source": [
    "## 3. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12307b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the current directory to path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from src.audio_features import AudioFeatureExtractor\n",
    "from src.data_processor import DataProcessor\n",
    "from src.models.audio_model import AudioEmotionModel\n",
    "from src.models.text_model import TextEmotionModel\n",
    "from src.models.multimodal_analyzer import MultimodalAnalyzer\n",
    "from src.utils import setup_logging, load_config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdb1ec",
   "metadata": {},
   "source": [
    "## 4. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17597c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show configuration\n",
    "config = load_config(\"config.yaml\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd14954",
   "metadata": {},
   "source": [
    "## 5. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab021a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio data\n",
    "data_processor = DataProcessor(config)\n",
    "features, labels = data_processor.process_data()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processor.split_data(features, labels)\n",
    "\n",
    "# Display data info\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b415533",
   "metadata": {},
   "source": [
    "## 6. Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361553ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the audio model\n",
    "audio_model = AudioEmotionModel(config)\n",
    "history = audio_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d9b26",
   "metadata": {},
   "source": [
    "## 7. Initialize Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text model\n",
    "text_model = TextEmotionModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c60dfc",
   "metadata": {},
   "source": [
    "## 8. Setup Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multimodal analyzer\n",
    "analyzer = MultimodalAnalyzer(audio_model, text_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd39f50",
   "metadata": {},
   "source": [
    "## 9. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample audio\n",
    "audio_path = \"data/ravdess/Actor_01/03-01-01-01-01-01-01.wav\"\n",
    "text = \"I'm feeling quite happy today.\"\n",
    "\n",
    "# Extract audio features\n",
    "feature_extractor = AudioFeatureExtractor(config)\n",
    "audio_features = feature_extractor.extract_features(audio_path)\n",
    "\n",
    "# Run multimodal analysis\n",
    "result = analyzer.analyze(audio_features, text)\n",
    "print(\"\\nMultimodal Analysis Results:\")\n",
    "print(f\"Detected Emotion: {result['emotion']}\")\n",
    "print(f\"Audio Emotion: {result['audio_emotion']}\")\n",
    "print(f\"Text Emotion: {result['text_emotion']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"Modality Agreement: {result['modality_agreement']}\")\n",
    "print(f\"Sarcasm Detected: {result['sarcasm_detected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0e009",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the audio model\n",
    "audio_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d4a07",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f984d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Save audio model\n",
    "audio_model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Models saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
