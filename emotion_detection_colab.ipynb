{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbceccf",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ac335",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "\n",
    "# Change working directory to the cloned repo\n",
    "import os\n",
    "os.chdir('speech-emotion-detection')\n",
    "!pwd\n",
    "\n",
    "# Install dependencies with specific versions compatible with Colab\n",
    "!pip install -q numpy==1.26.4 pandas==2.2.2 scikit-learn==1.2.2 matplotlib==3.7.1 tensorflow==2.15.0 librosa==0.10.1 transformers==4.35.2 soundfile==0.12.1\n",
    "\n",
    "# Verify installed versions\n",
    "!pip list | grep -E \"numpy|pandas|scikit-learn|matplotlib|tensorflow|librosa|transformers|soundfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d1898",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS dataset\n",
    "!wget -O ravdess.zip https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
    "!mkdir -p data/ravdess\n",
    "!unzip -q ravdess.zip -d data/ravdess\n",
    "!rm ravdess.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78daa730",
   "metadata": {},
   "source": [
    "## 3. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce48c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the repository structure\n",
    "!ls -la\n",
    "!find . -type d -name \"src\" -o -name \"source\" -o -name \"lib\"\n",
    "\n",
    "# Check current working directory and Python path\n",
    "import sys\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python path: {sys.path}\")\n",
    "\n",
    "# Create src directory if it doesn't exist (failsafe)\n",
    "!mkdir -p src\n",
    "\n",
    "# Check if we need to create module structure\n",
    "!test -d src/audio_features.py || test -d src/audio_features || echo \"Audio features module not found\"\n",
    "!test -d src/data_processor.py || test -d src/data_processor || echo \"Data processor module not found\"\n",
    "\n",
    "# Try different approaches to add the path\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b057819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue only after confirming module structure\n",
    "# First try to import directly\n",
    "try:\n",
    "    from src.audio_features import AudioFeatureExtractor\n",
    "    from src.data_processor import DataProcessor\n",
    "    from src.models.audio_model import AudioEmotionModel\n",
    "    from src.models.text_model import TextEmotionModel\n",
    "    from src.models.multimodal_analyzer import MultimodalAnalyzer\n",
    "    from src.utils import setup_logging, load_config\n",
    "    print(\"Modules successfully imported!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Module import error: {e}\")\n",
    "    print(\"\\nFallback to alternative import method:\")\n",
    "    # Try alternative approach if the repository structure is different\n",
    "    import importlib.util\n",
    "    import glob\n",
    "    \n",
    "    # Find Python files\n",
    "    py_files = glob.glob(\"**/*.py\", recursive=True)\n",
    "    print(f\"Python files found: {py_files}\")\n",
    "    \n",
    "    # Try to locate modules in a different structure\n",
    "    audio_features_path = next((path for path in py_files if \"audio_features\" in path), None)\n",
    "    data_processor_path = next((path for path in py_files if \"data_processor\" in path), None)\n",
    "    audio_model_path = next((path for path in py_files if \"audio_model\" in path), None)\n",
    "    text_model_path = next((path for path in py_files if \"text_model\" in path), None)\n",
    "    multimodal_path = next((path for path in py_files if \"multimodal\" in path), None)\n",
    "    utils_path = next((path for path in py_files if \"utils\" in path), None)\n",
    "    \n",
    "    print(f\"Found modules at: {audio_features_path}, {data_processor_path}, {audio_model_path}, {text_model_path}, {multimodal_path}, {utils_path}\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099b20e",
   "metadata": {},
   "source": [
    "## 4. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d860ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the config file\n",
    "!find . -name \"*.yaml\" -o -name \"*.yml\"\n",
    "\n",
    "# Load configuration - updated to handle dynamic path finding\n",
    "try:\n",
    "    config_path = \"config.yaml\"\n",
    "    config = load_config(config_path)\n",
    "    print(config)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading config: {e}\")\n",
    "    # Try to find and load config manually\n",
    "    import yaml\n",
    "    yaml_files = !find . -name \"*.yaml\" -o -name \"*.yml\"\n",
    "    if yaml_files:\n",
    "        with open(yaml_files[0], 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"Loaded config from {yaml_files[0]}\")\n",
    "        print(config)\n",
    "    else:\n",
    "        # Create a minimal default config if none exists\n",
    "        config = {\n",
    "            \"data\": {\"path\": \"data/ravdess\"},\n",
    "            \"model\": {\"type\": \"cnn\", \"params\": {\"units\": 64, \"dropout\": 0.5}}\n",
    "        }\n",
    "        print(\"Using default config:\")\n",
    "        print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d938e",
   "metadata": {},
   "source": [
    "## 5. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio data - wrapped in try/except for debugging\n",
    "try:\n",
    "    data_processor = DataProcessor(config)\n",
    "    features, labels = data_processor.process_data()\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = data_processor.split_data(features, labels)\n",
    "\n",
    "    # Display data info\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Testing data shape: {X_test.shape}\")\n",
    "    print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in data processing: {e}\")\n",
    "    # Create dummy data for testing\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Generate dummy features and labels\n",
    "    print(\"Generating dummy data for testing...\")\n",
    "    features = np.random.rand(100, 128)  # 100 samples, 128 features\n",
    "    labels = np.random.randint(0, 5, 100)  # 5 classes\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Dummy training data shape: {X_train.shape}\")\n",
    "    print(f\"Dummy testing data shape: {X_test.shape}\")\n",
    "    print(f\"Dummy class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dad41a",
   "metadata": {},
   "source": [
    "## 6. Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the audio model\n",
    "audio_model = AudioEmotionModel(config)\n",
    "history = audio_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d21875",
   "metadata": {},
   "source": [
    "## 7. Initialize Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6aafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text model\n",
    "text_model = TextEmotionModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a302993",
   "metadata": {},
   "source": [
    "## 8. Setup Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multimodal analyzer\n",
    "analyzer = MultimodalAnalyzer(audio_model, text_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44dbda",
   "metadata": {},
   "source": [
    "## 9. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23aa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample audio\n",
    "audio_path = \"data/ravdess/Actor_01/03-01-01-01-01-01-01.wav\"\n",
    "text = \"I'm feeling quite happy today.\"\n",
    "\n",
    "# Extract audio features\n",
    "feature_extractor = AudioFeatureExtractor(config)\n",
    "audio_features = feature_extractor.extract_features(audio_path)\n",
    "\n",
    "# Run multimodal analysis\n",
    "result = analyzer.analyze(audio_features, text)\n",
    "print(\"\\nMultimodal Analysis Results:\")\n",
    "print(f\"Detected Emotion: {result['emotion']}\")\n",
    "print(f\"Audio Emotion: {result['audio_emotion']}\")\n",
    "print(f\"Text Emotion: {result['text_emotion']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"Modality Agreement: {result['modality_agreement']}\")\n",
    "print(f\"Sarcasm Detected: {result['sarcasm_detected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bd51f",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the audio model\n",
    "audio_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ee34b",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Save audio model\n",
    "audio_model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Models saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
