{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa13a5b",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad059113",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "!cd speech-emotion-detection\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q numpy pandas scikit-learn matplotlib tensorflow librosa transformers soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00292a3",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb19b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS dataset\n",
    "!wget -O ravdess.zip https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
    "!mkdir -p data/ravdess\n",
    "!unzip -q ravdess.zip -d data/ravdess\n",
    "!rm ravdess.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93a3c7",
   "metadata": {},
   "source": [
    "## 3. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab924ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/speech-emotion-detection\")\n",
    "\n",
    "from src.audio_features import AudioFeatureExtractor\n",
    "from src.data_processor import DataProcessor\n",
    "from src.models.audio_model import AudioEmotionModel\n",
    "from src.models.text_model import TextEmotionModel\n",
    "from src.models.multimodal_analyzer import MultimodalAnalyzer\n",
    "from src.utils import setup_logging, load_config\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb7bfd",
   "metadata": {},
   "source": [
    "## 4. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60238dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and show configuration\n",
    "config = load_config(\"/content/speech-emotion-detection/config.yaml\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a61332",
   "metadata": {},
   "source": [
    "## 5. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio data\n",
    "data_processor = DataProcessor(config)\n",
    "features, labels = data_processor.process_data()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processor.split_data(features, labels)\n",
    "\n",
    "# Display data info\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f7042",
   "metadata": {},
   "source": [
    "## 6. Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the audio model\n",
    "audio_model = AudioEmotionModel(config)\n",
    "history = audio_model.train(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba7162",
   "metadata": {},
   "source": [
    "## 7. Initialize Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6afd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text model\n",
    "text_model = TextEmotionModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b728f",
   "metadata": {},
   "source": [
    "## 8. Setup Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d36881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the multimodal analyzer\n",
    "analyzer = MultimodalAnalyzer(audio_model, text_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629493dd",
   "metadata": {},
   "source": [
    "## 9. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample audio\n",
    "audio_path = \"/content/speech-emotion-detection/data/ravdess/Actor_01/03-01-01-01-01-01-01.wav\"\n",
    "text = \"I'm feeling quite happy today.\"\n",
    "\n",
    "# Extract audio features\n",
    "feature_extractor = AudioFeatureExtractor(config)\n",
    "audio_features = feature_extractor.extract_features(audio_path)\n",
    "\n",
    "# Run multimodal analysis\n",
    "result = analyzer.analyze(audio_features, text)\n",
    "print(\"\\nMultimodal Analysis Results:\")\n",
    "print(f\"Detected Emotion: {result['emotion']}\")\n",
    "print(f\"Audio Emotion: {result['audio_emotion']}\")\n",
    "print(f\"Text Emotion: {result['text_emotion']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "print(f\"Modality Agreement: {result['modality_agreement']}\")\n",
    "print(f\"Sarcasm Detected: {result['sarcasm_detected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db8d3c",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the audio model\n",
    "audio_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff5145",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a052f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Save audio model\n",
    "audio_model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Models saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
