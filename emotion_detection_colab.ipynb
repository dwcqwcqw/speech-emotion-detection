{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c665f8",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776ed68",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "\n",
    "# Change working directory to the cloned repo\n",
    "import os\n",
    "os.chdir('speech-emotion-detection')\n",
    "!pwd\n",
    "\n",
    "# Install dependencies with specific versions compatible with Colab\n",
    "!pip install -q numpy==1.26.4 pandas==2.2.2 scikit-learn==1.2.2 matplotlib==3.7.1 tensorflow==2.15.0 librosa==0.10.1 transformers==4.35.2 soundfile==0.12.1\n",
    "\n",
    "# Verify installed versions\n",
    "!pip list | grep -E \"numpy|pandas|scikit-learn|matplotlib|tensorflow|librosa|transformers|soundfile\"\n",
    "\n",
    "# Install additional packages if needed\n",
    "!pip install -q pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d4a9b",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f385ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS dataset\n",
    "!wget -O ravdess.zip https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
    "!mkdir -p data/ravdess\n",
    "!unzip -q ravdess.zip -d data/ravdess\n",
    "!rm ravdess.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331b799",
   "metadata": {},
   "source": [
    "## 3. Analyze Repository Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the repository structure\n",
    "!ls -la\n",
    "!find . -type f -name \"*.py\" | sort\n",
    "\n",
    "# Check current working directory and Python path\n",
    "import sys\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26caab9",
   "metadata": {},
   "source": [
    "## 4. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there's no src directory, we need to adapt our approach\n",
    "# Look at app directory since that likely contains the code\n",
    "!ls -la app/\n",
    "\n",
    "# Import necessary Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085349d9",
   "metadata": {},
   "source": [
    "## 5. Load or Create Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for config files in the repository\n",
    "!find . -name \"*.yaml\" -o -name \"*.yml\" -o -name \"*.json\" -o -name \"*.config\"\n",
    "\n",
    "# Create a default config if none exists\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"data/ravdess\",\n",
    "        \"test_size\": 0.2,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \"audio\": {\n",
    "        \"sample_rate\": 22050,\n",
    "        \"duration\": 3.0,\n",
    "        \"feature_type\": \"mfcc\",\n",
    "        \"n_mfcc\": 40\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"params\": {\n",
    "            \"units\": 128,\n",
    "            \"dropout\": 0.5,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 50\n",
    "        }\n",
    "    },\n",
    "    \"emotions\": [\"happy\", \"sad\", \"angry\", \"neutral\", \"fearful\"]\n",
    "}\n",
    "\n",
    "print(\"Using config:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50cf54",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99befe81",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import librosa for audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "\n",
    "# Define a function to extract features based on the run.py file\n",
    "def extract_features(file_path, config):\n",
    "    \"\"\"Extract audio features from a file.\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=config[\"audio\"][\"sample_rate\"], duration=config[\"audio\"][\"duration\"])\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config[\"audio\"][\"n_mfcc\"])\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        return mfccs_processed\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process data\n",
    "def process_data(config):\n",
    "    \"\"\"Process audio data and extract features.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    emotions = config[\"emotions\"]\n",
    "    data_path = config[\"data\"][\"path\"]\n",
    "    \n",
    "    # Find audio files\n",
    "    audio_files = glob.glob(f\"{data_path}/**/*.wav\", recursive=True)\n",
    "    print(f\"Found {len(audio_files)} audio files\")\n",
    "    \n",
    "    # Process a subset of files for demonstration (limit to 100 files)\n",
    "    sample_files = audio_files[:100] if len(audio_files) > 100 else audio_files\n",
    "    \n",
    "    for file_path in sample_files:\n",
    "        # Extract features\n",
    "        feature = extract_features(file_path, config)\n",
    "        if feature is not None:\n",
    "            features.append(feature)\n",
    "            \n",
    "            # For demonstration, assign random emotion labels\n",
    "            # In a real scenario, you would parse the filename or use a label file\n",
    "            label = np.random.randint(0, len(emotions))\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Extract features from audio files\n",
    "features, labels = process_data(config)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, \n",
    "    test_size=config[\"data\"][\"test_size\"], \n",
    "    random_state=config[\"data\"][\"random_state\"]\n",
    ")\n",
    "\n",
    "# Display data info\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a204bc",
   "metadata": {},
   "source": [
    "## 7. Build and Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model\n",
    "def create_model(config, input_shape):\n",
    "    \"\"\"Create an LSTM model for audio emotion recognition.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # LSTM layer\n",
    "    model.add(LSTM(\n",
    "        units=config[\"model\"][\"params\"][\"units\"],\n",
    "        input_shape=(input_shape[0], 1),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(config[\"model\"][\"params\"][\"dropout\"]))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=64))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(config[\"emotions\"]), activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"model\"][\"params\"][\"learning_rate\"]),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Reshape data for LSTM model\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Create model\n",
    "model = create_model(config, X_train.shape)\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    validation_data=(X_test_reshaped, y_test),\n",
    "    batch_size=config[\"model\"][\"params\"][\"batch_size\"],\n",
    "    epochs=10,  # Use fewer epochs for demonstration\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdba65",
   "metadata": {},
   "source": [
    "## 8. Text-based Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c555eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import transformers for text emotion analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text emotion classifier\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to analyze text emotion\n",
    "def analyze_text_emotion(text):\n",
    "    \"\"\"Analyze emotion from text using transformer model.\"\"\"\n",
    "    result = sentiment_analyzer(text)\n",
    "    \n",
    "    # Map sentiment labels to our emotion categories\n",
    "    # This is a simplistic mapping for demonstration\n",
    "    label = result[0][\"label\"]\n",
    "    score = result[0][\"score\"]\n",
    "    \n",
    "    if \"positive\" in label.lower():\n",
    "        emotion = \"happy\"\n",
    "    elif \"negative\" in label.lower():\n",
    "        emotion = \"sad\"  # or could be angry depending on context\n",
    "    else:\n",
    "        emotion = \"neutral\"\n",
    "    \n",
    "    return {\"emotion\": emotion, \"confidence\": score}\n",
    "\n",
    "# Test with sample text\n",
    "sample_texts = [\n",
    "    \"I'm feeling so happy today!\",\n",
    "    \"I'm so angry I could scream\",\n",
    "    \"I feel sad and disappointed\",\n",
    "    \"Just another normal day\",\n",
    "    \"That scared me so much\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    result = analyze_text_emotion(text)\n",
    "    print(f\"Text: '{text}' → Emotion: {result['emotion']} (Confidence: {result['confidence']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2f0cd",
   "metadata": {},
   "source": [
    "## 9. Multimodal Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple multimodal analyzer to combine audio and text\n",
    "class SimpleMultimodalAnalyzer:\n",
    "    def __init__(self, audio_model, config):\n",
    "        self.audio_model = audio_model\n",
    "        self.config = config\n",
    "        self.emotions = config[\"emotions\"]\n",
    "    \n",
    "    def analyze_audio(self, audio_features):\n",
    "        \"\"\"Predict emotion from audio features.\"\"\"\n",
    "        # Reshape for model input\n",
    "        features = audio_features.reshape(1, audio_features.shape[0], 1)\n",
    "        prediction = self.audio_model.predict(features, verbose=0)\n",
    "        \n",
    "        # Get predicted emotion and confidence\n",
    "        emotion_idx = np.argmax(prediction[0])\n",
    "        confidence = prediction[0][emotion_idx]\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": self.emotions[emotion_idx],\n",
    "            \"confidence\": float(confidence)\n",
    "        }\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Analyze emotion from text.\"\"\"\n",
    "        return analyze_text_emotion(text)\n",
    "    \n",
    "    def analyze(self, audio_features, text):\n",
    "        \"\"\"Combined analysis of audio and text.\"\"\"\n",
    "        audio_result = self.analyze_audio(audio_features)\n",
    "        text_result = self.analyze_text(text)\n",
    "        \n",
    "        # Check for agreement between modalities\n",
    "        agreement = audio_result[\"emotion\"] == text_result[\"emotion\"]\n",
    "        \n",
    "        # Calculate combined confidence\n",
    "        audio_weight = 0.6  # Give slightly more weight to audio\n",
    "        text_weight = 0.4\n",
    "        \n",
    "        # Detect potential sarcasm (when modalities disagree with high confidence)\n",
    "        sarcasm_detected = False\n",
    "        if not agreement and audio_result[\"confidence\"] > 0.7 and text_result[\"confidence\"] > 0.7:\n",
    "            sarcasm_detected = True\n",
    "        \n",
    "        # Determine final emotion (prefer audio if confident, otherwise use highest confidence)\n",
    "        if sarcasm_detected:\n",
    "            final_emotion = \"sarcastic\"\n",
    "            final_confidence = max(audio_result[\"confidence\"], text_result[\"confidence\"])\n",
    "        elif audio_result[\"confidence\"] > 0.7:\n",
    "            final_emotion = audio_result[\"emotion\"]\n",
    "            final_confidence = audio_result[\"confidence\"]\n",
    "        elif text_result[\"confidence\"] > 0.7:\n",
    "            final_emotion = text_result[\"emotion\"]\n",
    "            final_confidence = text_result[\"confidence\"]\n",
    "        else:\n",
    "            # Use weighted confidence\n",
    "            if audio_result[\"confidence\"] * audio_weight > text_result[\"confidence\"] * text_weight:\n",
    "                final_emotion = audio_result[\"emotion\"]\n",
    "            else:\n",
    "                final_emotion = text_result[\"emotion\"]\n",
    "            \n",
    "            final_confidence = (audio_result[\"confidence\"] * audio_weight) + (text_result[\"confidence\"] * text_weight)\n",
    "        \n",
    "        return {\n",
    "            \"emotion\": final_emotion,\n",
    "            \"audio_emotion\": audio_result[\"emotion\"],\n",
    "            \"text_emotion\": text_result[\"emotion\"],\n",
    "            \"confidence\": final_confidence,\n",
    "            \"modality_agreement\": agreement,\n",
    "            \"sarcasm_detected\": sarcasm_detected\n",
    "        }\n",
    "\n",
    "# Create the multimodal analyzer\n",
    "multimodal_analyzer = SimpleMultimodalAnalyzer(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32094b46",
   "metadata": {},
   "source": [
    "## 10. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample audio file and text\n",
    "# Find a sample audio file\n",
    "sample_files = glob.glob(\"data/ravdess/**/*.wav\", recursive=True)\n",
    "\n",
    "if sample_files:\n",
    "    # Extract features from a sample file\n",
    "    sample_audio_path = sample_files[0]\n",
    "    print(f\"Using sample audio: {sample_audio_path}\")\n",
    "    \n",
    "    sample_features = extract_features(sample_audio_path, config)\n",
    "    \n",
    "    # Define sample texts with different emotions\n",
    "    sample_text_pairs = [\n",
    "        (\"I'm feeling really happy today!\", \"matching\"),\n",
    "        (\"I'm so angry right now!\", \"conflicting\"),\n",
    "        (\"I feel rather neutral about this\", \"neutral\"),\n",
    "        (\"This makes me so sad\", \"conflicting\")\n",
    "    ]\n",
    "    \n",
    "    # Test with different text samples\n",
    "    for text, description in sample_text_pairs:\n",
    "        print(f\"\\nTesting with {description} text: '{text}'\")\n",
    "        result = multimodal_analyzer.analyze(sample_features, text)\n",
    "        \n",
    "        print(\"Multimodal Analysis Results:\")\n",
    "        print(f\"Detected Emotion: {result['emotion']}\")\n",
    "        print(f\"Audio Emotion: {result['audio_emotion']}\")\n",
    "        print(f\"Text Emotion: {result['text_emotion']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"Modality Agreement: {result['modality_agreement']}\")\n",
    "        print(f\"Sarcasm Detected: {result['sarcasm_detected']}\")\n",
    "else:\n",
    "    print(\"No sample audio files found. Make sure the dataset was downloaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229eda7b",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create directory for models\n",
    "!mkdir -p \"/content/drive/MyDrive/emotion_detection_models\"\n",
    "\n",
    "# Save audio model\n",
    "model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Model saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
