{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e564030",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Detection System\n",
    "\n",
    "This notebook provides a complete setup for running the multimodal emotion detection system in Google Colab. The system combines speech prosody analysis with text analysis to detect emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bbbce",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da24d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dwcqwcqw/speech-emotion-detection.git\n",
    "\n",
    "# Change working directory to the cloned repo\n",
    "import os\n",
    "os.chdir('speech-emotion-detection')\n",
    "!pwd\n",
    "\n",
    "# Install dependencies with specific versions compatible with Colab\n",
    "!pip install -q numpy==1.26.4 pandas==2.2.2 scikit-learn==1.2.2 matplotlib==3.7.1 tensorflow==2.15.0 librosa==0.10.1 transformers==4.35.2 soundfile==0.12.1\n",
    "\n",
    "# Verify installed versions\n",
    "!pip list | grep -E \"numpy|pandas|scikit-learn|matplotlib|tensorflow|librosa|transformers|soundfile\"\n",
    "\n",
    "# Install additional packages if needed\n",
    "!pip install -q pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72c778",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download RAVDESS dataset\n",
    "!wget -O ravdess.zip https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\n",
    "!mkdir -p data/ravdess\n",
    "!unzip -q ravdess.zip -d data/ravdess\n",
    "!rm ravdess.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b730e9e",
   "metadata": {},
   "source": [
    "## 3. Analyze Repository Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore repository directories\n",
    "!ls -la\n",
    "!echo \"\\nChecking for src directory:\"\n",
    "!ls -la src 2>/dev/null || echo \"src directory not found\"\n",
    "!echo \"\\nChecking app directory:\"\n",
    "!ls -la app/ 2>/dev/null || echo \"app directory not found\"\n",
    "!echo \"\\nListing Python files:\" \n",
    "!find . -maxdepth 3 -type f -name \"*.py\" | sort\n",
    "\n",
    "# Create src directory if needed (for module imports)\n",
    "!mkdir -p src\n",
    "\n",
    "# Check current working directory and Python path\n",
    "import sys\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python path: {sys.path}\")\n",
    "\n",
    "# Try both app and src paths\n",
    "MODULE_BASE = 'app' if os.path.isdir('app') else 'src'\n",
    "print(f\"Using module base directory: {MODULE_BASE}\")\n",
    "sys.path.append(os.path.join(os.getcwd(), MODULE_BASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee60cac",
   "metadata": {},
   "source": [
    "## 4. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618561c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try importing from potential modules\n",
    "try:\n",
    "    # Try potential module structures\n",
    "    if os.path.isdir('app'):\n",
    "        print(\"Attempting to import from app directory...\")\n",
    "        import app\n",
    "        from app.models import AudioEmotionModel, TextEmotionModel\n",
    "        from app.utils import setup_logging, load_config\n",
    "        print(\"Successfully imported from app directory.\")\n",
    "    elif os.path.isdir('src'):\n",
    "        print(\"Attempting to import from src directory...\")\n",
    "        import src\n",
    "        from src.audio_features import AudioFeatureExtractor\n",
    "        from src.data_processor import DataProcessor\n",
    "        from src.models.audio_model import AudioEmotionModel\n",
    "        from src.models.text_model import TextEmotionModel\n",
    "        from src.models.multimodal_analyzer import MultimodalAnalyzer\n",
    "        from src.utils import setup_logging, load_config\n",
    "        print(\"Successfully imported from src directory.\")\n",
    "    \n",
    "    # Check if we can find run.py's functionality\n",
    "    print(\"Checking run.py for importing functionality...\")\n",
    "    import run\n",
    "    print(\"Successfully imported run.py\")\n",
    "    \n",
    "    # If we get here, we successfully imported some modules\n",
    "    print(\"Module imports successful!\")\n",
    "    USE_IMPORTED_MODULES = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not import modules: {e}\")\n",
    "    print(\"Falling back to standalone implementation...\")\n",
    "    USE_IMPORTED_MODULES = False\n",
    "\n",
    "# Import necessary Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af099480",
   "metadata": {},
   "source": [
    "## 5. Load or Create Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for config files in the repository\n",
    "!find . -name \"*.yaml\" -o -name \"*.yml\" -o -name \"*.json\" -o -name \"*.config\"\n",
    "\n",
    "# Try to load config from repository\n",
    "try:\n",
    "    # Try to import config\n",
    "    if 'load_config' in locals():\n",
    "        print(\"Using imported load_config function...\")\n",
    "        config = load_config(\"config.yaml\")\n",
    "        print(\"Config loaded successfully:\")\n",
    "        print(config)\n",
    "    else:\n",
    "        # Try to find and load config manually\n",
    "        yaml_files = !find . -name \"*.yaml\" -o -name \"*.yml\"\n",
    "        if yaml_files:\n",
    "            print(f\"Found config file: {yaml_files[0]}\")\n",
    "            with open(yaml_files[0], 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            print(\"Config loaded successfully:\")\n",
    "            print(json.dumps(config, indent=2))\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No config files found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading config: {e}\")\n",
    "    print(\"Creating default config...\")\n",
    "    \n",
    "    # Create a default config if none exists\n",
    "    config = {\n",
    "        \"data\": {\n",
    "            \"path\": \"data/ravdess\",\n",
    "            \"test_size\": 0.2,\n",
    "            \"random_state\": 42\n",
    "        },\n",
    "        \"audio\": {\n",
    "            \"sample_rate\": 22050,\n",
    "            \"duration\": 3.0,\n",
    "            \"feature_type\": \"mfcc\",\n",
    "            \"n_mfcc\": 40\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"params\": {\n",
    "                \"units\": 128,\n",
    "                \"dropout\": 0.5,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"batch_size\": 32,\n",
    "                \"epochs\": 50\n",
    "            }\n",
    "        },\n",
    "        \"emotions\": [\"happy\", \"sad\", \"angry\", \"neutral\", \"fearful\"]\n",
    "    }\n",
    "    \n",
    "    print(\"Using default config:\")\n",
    "    print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34b1c4",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ee89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to use imported modules or standalone code\n",
    "if USE_IMPORTED_MODULES and 'AudioFeatureExtractor' in locals() and 'DataProcessor' in locals():\n",
    "    print(\"Using imported modules for feature extraction and data processing...\")\n",
    "    \n",
    "    # Use imported modules\n",
    "    feature_extractor = AudioFeatureExtractor(config)\n",
    "    data_processor = DataProcessor(config)\n",
    "    \n",
    "    # Process data using imported modules\n",
    "    features, labels = data_processor.process_data()\n",
    "    X_train, X_test, y_train, y_test = data_processor.split_data(features, labels)\n",
    "    \n",
    "else:\n",
    "    print(\"Using standalone implementation for feature extraction and data processing...\")\n",
    "    \n",
    "    # Import librosa for audio processing\n",
    "    import librosa\n",
    "    import librosa.display\n",
    "    import glob\n",
    "    \n",
    "    # Define a function to extract features\n",
    "    def extract_features(file_path, config):\n",
    "        \"\"\"Extract audio features from a file.\"\"\"\n",
    "        try:\n",
    "            # Load audio file\n",
    "            y, sr = librosa.load(file_path, sr=config[\"audio\"][\"sample_rate\"], duration=config[\"audio\"][\"duration\"])\n",
    "            \n",
    "            # Extract MFCCs\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config[\"audio\"][\"n_mfcc\"])\n",
    "            mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "            \n",
    "            return mfccs_processed\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features from {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Function to process data\n",
    "    def process_data(config):\n",
    "        \"\"\"Process audio data and extract features.\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        emotions = config[\"emotions\"]\n",
    "        data_path = config[\"data\"][\"path\"]\n",
    "        \n",
    "        # Find audio files\n",
    "        audio_files = glob.glob(f\"{data_path}/**/*.wav\", recursive=True)\n",
    "        print(f\"Found {len(audio_files)} audio files\")\n",
    "        \n",
    "        # Process a subset of files for demonstration (limit to 100 files)\n",
    "        sample_files = audio_files[:100] if len(audio_files) > 100 else audio_files\n",
    "        \n",
    "        for file_path in sample_files:\n",
    "            # Extract features\n",
    "            feature = extract_features(file_path, config)\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                \n",
    "                # For demonstration, assign random emotion labels\n",
    "                # In a real scenario, you would parse the filename or use a label file\n",
    "                label = np.random.randint(0, len(emotions))\n",
    "                labels.append(label)\n",
    "        \n",
    "        return np.array(features), np.array(labels)\n",
    "    \n",
    "    # Extract features from audio files\n",
    "    features, labels = process_data(config)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, \n",
    "        test_size=config[\"data\"][\"test_size\"], \n",
    "        random_state=config[\"data\"][\"random_state\"]\n",
    "    )\n",
    "\n",
    "# Display data info\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c498b3d",
   "metadata": {},
   "source": [
    "## 7. Build and Train Audio Emotion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to use imported model or create one\n",
    "if USE_IMPORTED_MODULES and 'AudioEmotionModel' in locals():\n",
    "    print(\"Using imported AudioEmotionModel...\")\n",
    "    \n",
    "    # Use imported audio model\n",
    "    audio_model = AudioEmotionModel(config)\n",
    "    history = audio_model.train(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "else:\n",
    "    print(\"Creating standalone model...\")\n",
    "    \n",
    "    # Function to create a model\n",
    "    def create_model(config, input_shape):\n",
    "        \"\"\"Create an LSTM model for audio emotion recognition.\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # LSTM layer\n",
    "        model.add(LSTM(\n",
    "            units=config[\"model\"][\"params\"][\"units\"],\n",
    "            input_shape=(input_shape[0], 1),\n",
    "            return_sequences=True\n",
    "        ))\n",
    "        model.add(Dropout(config[\"model\"][\"params\"][\"dropout\"]))\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        model.add(LSTM(units=64))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(len(config[\"emotions\"]), activation='softmax'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"model\"][\"params\"][\"learning_rate\"]),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Reshape data for LSTM model\n",
    "    X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(config, X_train.shape)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_reshaped, y_train,\n",
    "        validation_data=(X_test_reshaped, y_test),\n",
    "        batch_size=config[\"model\"][\"params\"][\"batch_size\"],\n",
    "        epochs=10,  # Use fewer epochs for demonstration\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b46cc",
   "metadata": {},
   "source": [
    "## 8. Text-based Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to use imported text model or create one\n",
    "if USE_IMPORTED_MODULES and 'TextEmotionModel' in locals():\n",
    "    print(\"Using imported TextEmotionModel...\")\n",
    "    \n",
    "    # Use imported text model\n",
    "    text_model = TextEmotionModel(config)\n",
    "    \n",
    "    # Define analyze_text_emotion function to match interface\n",
    "    def analyze_text_emotion(text):\n",
    "        return text_model.predict(text)\n",
    "    \n",
    "else:\n",
    "    print(\"Creating standalone text analyzer...\")\n",
    "    \n",
    "    # Import transformers for text emotion analysis\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Create a text emotion classifier\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    # Function to analyze text emotion\n",
    "    def analyze_text_emotion(text):\n",
    "        \"\"\"Analyze emotion from text using transformer model.\"\"\"\n",
    "        result = sentiment_analyzer(text)\n",
    "        \n",
    "        # Map sentiment labels to our emotion categories\n",
    "        # This is a simplistic mapping for demonstration\n",
    "        label = result[0][\"label\"]\n",
    "        score = result[0][\"score\"]\n",
    "        \n",
    "        if \"positive\" in label.lower():\n",
    "            emotion = \"happy\"\n",
    "        elif \"negative\" in label.lower():\n",
    "            emotion = \"sad\"  # or could be angry depending on context\n",
    "        else:\n",
    "            emotion = \"neutral\"\n",
    "        \n",
    "        return {\"emotion\": emotion, \"confidence\": score}\n",
    "\n",
    "# Test with sample text\n",
    "sample_texts = [\n",
    "    \"I'm feeling so happy today!\",\n",
    "    \"I'm so angry I could scream\",\n",
    "    \"I feel sad and disappointed\",\n",
    "    \"Just another normal day\",\n",
    "    \"That scared me so much\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    result = analyze_text_emotion(text)\n",
    "    print(f\"Text: '{text}' â†’ Emotion: {result['emotion']} (Confidence: {result['confidence']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1c9de",
   "metadata": {},
   "source": [
    "## 9. Multimodal Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74519f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to use imported multimodal analyzer or create one\n",
    "if USE_IMPORTED_MODULES and 'MultimodalAnalyzer' in locals():\n",
    "    print(\"Using imported MultimodalAnalyzer...\")\n",
    "    \n",
    "    # Use imported multimodal analyzer\n",
    "    multimodal_analyzer = MultimodalAnalyzer(audio_model, text_model, config)\n",
    "    \n",
    "else:\n",
    "    print(\"Creating standalone multimodal analyzer...\")\n",
    "    \n",
    "    # Create a simple multimodal analyzer to combine audio and text\n",
    "    class SimpleMultimodalAnalyzer:\n",
    "        def __init__(self, audio_model, config):\n",
    "            self.audio_model = audio_model\n",
    "            self.config = config\n",
    "            self.emotions = config[\"emotions\"]\n",
    "        \n",
    "        def analyze_audio(self, audio_features):\n",
    "            \"\"\"Predict emotion from audio features.\"\"\"\n",
    "            # Reshape for model input\n",
    "            features = audio_features.reshape(1, audio_features.shape[0], 1)\n",
    "            prediction = self.audio_model.predict(features, verbose=0)\n",
    "            \n",
    "            # Get predicted emotion and confidence\n",
    "            emotion_idx = np.argmax(prediction[0])\n",
    "            confidence = prediction[0][emotion_idx]\n",
    "            \n",
    "            return {\n",
    "                \"emotion\": self.emotions[emotion_idx],\n",
    "                \"confidence\": float(confidence)\n",
    "            }\n",
    "        \n",
    "        def analyze_text(self, text):\n",
    "            \"\"\"Analyze emotion from text.\"\"\"\n",
    "            return analyze_text_emotion(text)\n",
    "        \n",
    "        def analyze(self, audio_features, text):\n",
    "            \"\"\"Combined analysis of audio and text.\"\"\"\n",
    "            audio_result = self.analyze_audio(audio_features)\n",
    "            text_result = self.analyze_text(text)\n",
    "            \n",
    "            # Check for agreement between modalities\n",
    "            agreement = audio_result[\"emotion\"] == text_result[\"emotion\"]\n",
    "            \n",
    "            # Calculate combined confidence\n",
    "            audio_weight = 0.6  # Give slightly more weight to audio\n",
    "            text_weight = 0.4\n",
    "            \n",
    "            # Detect potential sarcasm (when modalities disagree with high confidence)\n",
    "            sarcasm_detected = False\n",
    "            if not agreement and audio_result[\"confidence\"] > 0.7 and text_result[\"confidence\"] > 0.7:\n",
    "                sarcasm_detected = True\n",
    "            \n",
    "            # Determine final emotion (prefer audio if confident, otherwise use highest confidence)\n",
    "            if sarcasm_detected:\n",
    "                final_emotion = \"sarcastic\"\n",
    "                final_confidence = max(audio_result[\"confidence\"], text_result[\"confidence\"])\n",
    "            elif audio_result[\"confidence\"] > 0.7:\n",
    "                final_emotion = audio_result[\"emotion\"]\n",
    "                final_confidence = audio_result[\"confidence\"]\n",
    "            elif text_result[\"confidence\"] > 0.7:\n",
    "                final_emotion = text_result[\"emotion\"]\n",
    "                final_confidence = text_result[\"confidence\"]\n",
    "            else:\n",
    "                # Use weighted confidence\n",
    "                if audio_result[\"confidence\"] * audio_weight > text_result[\"confidence\"] * text_weight:\n",
    "                    final_emotion = audio_result[\"emotion\"]\n",
    "                else:\n",
    "                    final_emotion = text_result[\"emotion\"]\n",
    "                \n",
    "                final_confidence = (audio_result[\"confidence\"] * audio_weight) + (text_result[\"confidence\"] * text_weight)\n",
    "            \n",
    "            return {\n",
    "                \"emotion\": final_emotion,\n",
    "                \"audio_emotion\": audio_result[\"emotion\"],\n",
    "                \"text_emotion\": text_result[\"emotion\"],\n",
    "                \"confidence\": final_confidence,\n",
    "                \"modality_agreement\": agreement,\n",
    "                \"sarcasm_detected\": sarcasm_detected\n",
    "            }\n",
    "    \n",
    "    # Create the multimodal analyzer\n",
    "    if 'model' in locals():\n",
    "        multimodal_analyzer = SimpleMultimodalAnalyzer(model, config)\n",
    "    else:\n",
    "        multimodal_analyzer = SimpleMultimodalAnalyzer(audio_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0233d",
   "metadata": {},
   "source": [
    "## 10. Test with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0507dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample audio file and text\n",
    "# Find a sample audio file\n",
    "sample_files = glob.glob(\"data/ravdess/**/*.wav\", recursive=True)\n",
    "\n",
    "if sample_files:\n",
    "    # Extract features from a sample file\n",
    "    sample_audio_path = sample_files[0]\n",
    "    print(f\"Using sample audio: {sample_audio_path}\")\n",
    "    \n",
    "    # Extract features for the sample file\n",
    "    if USE_IMPORTED_MODULES and 'AudioFeatureExtractor' in locals():\n",
    "        sample_features = feature_extractor.extract_features(sample_audio_path)\n",
    "    else:\n",
    "        sample_features = extract_features(sample_audio_path, config)\n",
    "    \n",
    "    # Define sample texts with different emotions\n",
    "    sample_text_pairs = [\n",
    "        (\"I'm feeling really happy today!\", \"matching\"),\n",
    "        (\"I'm so angry right now!\", \"conflicting\"),\n",
    "        (\"I feel rather neutral about this\", \"neutral\"),\n",
    "        (\"This makes me so sad\", \"conflicting\")\n",
    "    ]\n",
    "    \n",
    "    # Test with different text samples\n",
    "    for text, description in sample_text_pairs:\n",
    "        print(f\"\\nTesting with {description} text: '{text}'\")\n",
    "        result = multimodal_analyzer.analyze(sample_features, text)\n",
    "        \n",
    "        print(\"Multimodal Analysis Results:\")\n",
    "        print(f\"Detected Emotion: {result['emotion']}\")\n",
    "        print(f\"Audio Emotion: {result['audio_emotion']}\")\n",
    "        print(f\"Text Emotion: {result['text_emotion']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"Modality Agreement: {result['modality_agreement']}\")\n",
    "        print(f\"Sarcasm Detected: {result['sarcasm_detected']}\")\n",
    "else:\n",
    "    print(\"No sample audio files found. Make sure the dataset was downloaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84ce7",
   "metadata": {},
   "source": [
    "## 11. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cafd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create directory for models\n",
    "!mkdir -p \"/content/drive/MyDrive/emotion_detection_models\"\n",
    "\n",
    "# Save model\n",
    "if 'model' in locals():\n",
    "    model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "else:\n",
    "    audio_model.save(\"/content/drive/MyDrive/emotion_detection_models/audio_model\")\n",
    "print(\"Model saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
